{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O56jiwblPcTD",
        "outputId": "63e020c0-b248-48a4-a0b8-8cad6732060f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "from nltk.stem.lancaster import LancasterStemmer\n",
        "stemmer = LancasterStemmer()\n",
        "import tensorflow as tf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L6vDHxjWPeLB",
        "outputId": "9900558b-b76b-46b8-cbb4-b31e7fab2bb0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting git+https://github.com/MihaMarkic/tflearn.git@fix/is_sequence_missing\n",
            "  Cloning https://github.com/MihaMarkic/tflearn.git (to revision fix/is_sequence_missing) to /tmp/pip-req-build-ptcm0ukl\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/MihaMarkic/tflearn.git /tmp/pip-req-build-ptcm0ukl\n",
            "  Running command git checkout -b fix/is_sequence_missing --track origin/fix/is_sequence_missing\n",
            "  Switched to a new branch 'fix/is_sequence_missing'\n",
            "  Branch 'fix/is_sequence_missing' set up to track remote branch 'fix/is_sequence_missing' from 'origin'.\n",
            "  Resolved https://github.com/MihaMarkic/tflearn.git to commit 6472b8588e758ff4a33a2764d4ee638bbd0e42f0\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from tflearn==0.5.0) (1.25.2)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from tflearn==0.5.0) (1.16.0)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.10/dist-packages (from tflearn==0.5.0) (9.4.0)\n",
            "Building wheels for collected packages: tflearn\n",
            "  Building wheel for tflearn (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for tflearn: filename=tflearn-0.5.0-py3-none-any.whl size=130659 sha256=3da21c7f3412b31fe4417df6cd903cb6c852ad5b21e59a4971745d9e2bda480b\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-x3s_e23a/wheels/4d/8c/0c/2159783f8dfd53bdaf4e59cd3990607ac8d5d912ae6de88296\n",
            "Successfully built tflearn\n",
            "Installing collected packages: tflearn\n",
            "Successfully installed tflearn-0.5.0\n"
          ]
        }
      ],
      "source": [
        "pip install git+https://github.com/MihaMarkic/tflearn.git@fix/is_sequence_missing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "vRqgJ4OJPg8P",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fbea7b26-c03b-4deb-b925-91206f76bc3f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.10/dist-packages/tensorflow/python/compat/v2_compat.py:108: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "non-resource variables are not supported in the long term\n"
          ]
        }
      ],
      "source": [
        "import numpy\n",
        "import tensorflow as tf\n",
        "import tflearn\n",
        "import random\n",
        "import json\n",
        "import pickle\n",
        "from time import sleep"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "guKFhfd3PnwA",
        "outputId": "f356149d-525b-408b-88aa-9600747b9d6e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Step: 6399  | total loss: \u001b[1m\u001b[32m0.44847\u001b[0m\u001b[0m | time: 0.199s\n",
            "| Adam | epoch: 200 | loss: 0.44847 - acc: 0.8707 -- iter: 248/256\n",
            "Training Step: 6400  | total loss: \u001b[1m\u001b[32m0.43654\u001b[0m\u001b[0m | time: 0.207s\n",
            "| Adam | epoch: 200 | loss: 0.43654 - acc: 0.8953 -- iter: 256/256\n",
            "--\n",
            "F1 Score (Training): 0.9422\n",
            "Test Loss: 0.1538\n",
            "Hi, How can i help you ?\n",
            "You: how are you\n",
            "Hello! How may I assist you?\n",
            "You: what is the time \n",
            "Right now, it is [current time] [current time zone].\n",
            "You: x\n",
            "I don't understand!\n",
            "You: how are you\n",
            "Hello! How can I assist you?\n",
            "You: boring\n",
            "I don't understand!\n",
            "You: what is the time \n",
            "Right now, it is [current time] [current time zone].\n"
          ]
        }
      ],
      "source": [
        "from sklearn.metrics import f1_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "\n",
        "with open(\"intents2.json\") as file:\n",
        "    data = json.load(file)\n",
        "\n",
        "try:\n",
        "    with open(\"data.pickle\", \"rb\") as f:\n",
        "        words, labels, training, output = pickle.load(f)\n",
        "except:\n",
        "    words = []\n",
        "    labels = []\n",
        "    docs_x = []\n",
        "    docs_y = []\n",
        "    for intent in data [\"intents\"]:\n",
        "        for pattern in intent[\"patterns\"]:\n",
        "            wrds = nltk.word_tokenize(pattern)\n",
        "            words.extend(wrds)\n",
        "            docs_x.append(wrds)\n",
        "            docs_y.append(intent[\"tag\"])\n",
        "\n",
        "        if intent[\"tag\"] not in labels:\n",
        "            labels.append(intent[\"tag\"])\n",
        "\n",
        "    words = [stemmer.stem(w.lower()) for w in words if w != \"?\"]\n",
        "    words = sorted(list(set(words)))\n",
        "\n",
        "    labels = sorted(labels)\n",
        "\n",
        "    training = []\n",
        "    output = []\n",
        "\n",
        "    out_empty = [0 for _ in range(len(labels))]\n",
        "\n",
        "    for x, doc in enumerate(docs_x):\n",
        "        bag = []\n",
        "\n",
        "        wrds = [stemmer.stem(w) for w in doc]\n",
        "\n",
        "        for w in words:\n",
        "            if w in wrds:\n",
        "                bag.append(1)\n",
        "            else:\n",
        "                bag.append(0)\n",
        "\n",
        "\n",
        "        output_row = out_empty[:]\n",
        "        output_row[labels.index(docs_y[x])] = 1\n",
        "\n",
        "        training.append(bag)\n",
        "        output.append(output_row)\n",
        "\n",
        "\n",
        "    training = numpy.array(training)\n",
        "    output = numpy.array(output)\n",
        "    X_train, X_test, y_train, y_test = train_test_split(training, output, test_size=0.2)\n",
        "\n",
        "    with open(\"data.pickle\", \"wb\") as f:\n",
        "        pickle.dump((words, labels, training, output), f)\n",
        "\n",
        "tf.compat.v1.reset_default_graph()\n",
        "\n",
        "net = tflearn.input_data(shape=[None, len(X_train[0])])\n",
        "net = tflearn.fully_connected(net, 20)\n",
        "net = tflearn.fully_connected(net, 20)\n",
        "net = tflearn.fully_connected(net, 20)\n",
        "net = tflearn.fully_connected(net, len(y_train[0]), activation = \"softmax\")\n",
        "net = tflearn.regression(net)\n",
        "\n",
        "model = tflearn.DNN(net)\n",
        "model.fit(X_train, y_train, n_epoch=200, batch_size=8, show_metric=True)\n",
        "model.save(\"model.tflearn\")\n",
        "y_train_pred = model.predict(X_train)\n",
        "f1_train = f1_score(y_train.argmax(axis=1), y_train_pred.argmax(axis=1), average='weighted')\n",
        "print(f\"F1 Score (Training): {f1_train:.4f}\")\n",
        "\n",
        "test_loss = model.evaluate(X_test, y_test)[0]\n",
        "print(f\"Test Loss: {test_loss:.4f}\")\n",
        "def bag_of_words(s, words):\n",
        "    bag = [0 for _ in range(len(words))]\n",
        "\n",
        "    s_words = nltk.word_tokenize(s)\n",
        "    s_words = [stemmer.stem(word.lower()) for word in s_words]\n",
        "\n",
        "    for se in s_words:\n",
        "        for i, w in enumerate(words):\n",
        "            if w == se:\n",
        "                bag[i] = 1\n",
        "\n",
        "    return numpy.array(bag)\n",
        "\n",
        "def chat():\n",
        "    print(\"Hi, How can i help you ?\")\n",
        "    while True:\n",
        "        inp = input(\"You: \")\n",
        "        if inp.lower() == \"quit\":\n",
        "            break\n",
        "\n",
        "        results = model.predict([bag_of_words(inp, words)])[0]\n",
        "        results_index = numpy.argmax(results)\n",
        "        tag = labels[results_index]\n",
        "        if results[results_index] > 0.8:\n",
        "            for tg in data[\"intents\"]:\n",
        "                if tg['tag'] == tag:\n",
        "                    responses = tg['responses']\n",
        "            sleep(3)\n",
        "            Bot = random.choice(responses)\n",
        "            print(Bot)\n",
        "        else:\n",
        "            print(\"I don't understand!\")\n",
        "chat()"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "UgpCzPxtAVh4"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}